{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP2lZQkzvPq6y44SvEyCEYJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dwdb/tensorflow2.0-tutorial/blob/master/text/text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QlTI3o2J6b4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1CxElZvJ_b3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "58c923dc-a19e-464b-d071-e5297adb8a0d"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVPoTR1iKAx8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4c78a49d-a26f-40dc-f213-db2a3d972cbd"
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOBkT851KNKu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "90b5ee96-fd55-4d8f-cabb-ec7244169eeb"
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS9NSF6nKO28",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9f782bb7-d4ae-4621-fe3b-b0dd2a1de8ba"
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EzYkt6PKaaT",
        "colab_type": "text"
      },
      "source": [
        "## Process the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcdntafjKXV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoYLPk38KZCV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "67cd77df-7d5e-4ef0-fe87-6a0098ebf152"
      },
      "source": [
        "# Show how the first 13 characters from the text are mapped to integers\n",
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen' ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caoNppcdKpkO",
        "colab_type": "text"
      },
      "source": [
        "## Create training examples and targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSBAE_qWKfkl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "3d7ec106-1462-48a6-a116-4aff56ce2170"
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qUs_8YrKs25",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "edad3946-6a4d-428b-a388-8421d6166530"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=False)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uq-GQDaGKvI_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "90119371-ed7e-4872-b34a-255da471139f"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsTUQOddK2Vt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8YyRpZIL-i6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "56063d84-3205-45e7-93be-26689820908b"
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar80NAoKMMtq",
        "colab_type": "text"
      },
      "source": [
        "## Create training batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajRqMTXKMEG-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5eb2f89f-8983-4b4d-a3c2-5b518ac2f2e2"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRdgm4KMf6H_",
        "colab_type": "text"
      },
      "source": [
        "# drop_remainder作用，是否返回长度不满足batch_size的batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAFq10jmgPVW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "72425565-410a-4d7b-8d7c-c9c6e6bed4cb"
      },
      "source": [
        "dataset_ = tf.data.Dataset.from_tensor_slices([[1,2],[3,4],[5,6],[7,8]])\n",
        "\n",
        "dataset_ = dataset_.shuffle(100).batch(3, drop_remainder=True).repeat(10)\n",
        "\n",
        "for i in dataset_.take(3):\n",
        "  print(i)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[7 8]\n",
            " [3 4]\n",
            " [5 6]], shape=(3, 2), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[1 2]\n",
            " [5 6]\n",
            " [3 4]], shape=(3, 2), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[1 2]\n",
            " [3 4]\n",
            " [7 8]], shape=(3, 2), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFLCXB6YNVcl",
        "colab_type": "text"
      },
      "source": [
        "## Build The Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_FHKKvcNjck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKULIWRcNqtW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyBtMmqdNsGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4RlvlY4OQ42",
        "colab_type": "text"
      },
      "source": [
        "For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoA7eB31N6Dj",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://www.tensorflow.org/tutorials/text/images/text_generation_training.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkWz09aoOTjt",
        "colab_type": "text"
      },
      "source": [
        "## Try the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzI4FZOoOg9f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "50cd4fae-3dc8-4456-ca42-35f6deb612fd"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDE1pRutOiT3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "39a33e42-cc9c-4ea0-fdea-0470fb3e1cff"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HK_XRwnOkW0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "738214d9-6b6e-489e-8045-240e2eb664c2"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "\n",
        "sampled_indices"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([57, 38, 27, 50, 18, 44, 60,  2, 27,  7, 48, 29, 17, 23, 16, 49, 40,\n",
              "       30, 22, 52, 20, 44, 24, 41, 50, 25,  9, 45,  6, 10, 16, 11, 60, 58,\n",
              "        8, 64, 14, 53,  1,  4,  4, 22, 17, 32, 44, 61, 62, 21, 59, 62, 22,\n",
              "       36,  7, 27, 19, 51, 34, 18, 59, 49, 54, 29, 21, 28, 38, 13, 49, 30,\n",
              "       44, 16, 47, 60, 59, 16, 58,  6, 41, 61, 44, 47, 24, 19, 38, 58, 37,\n",
              "       50, 21,  2,  5, 64, 44, 59, 39,  9, 18, 11, 36, 37, 40, 44])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxykQ06kOq0W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "f3cda042-5f70-40d7-d0c1-e7aee8f6eec8"
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices])))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " ' the commonalty.\\n\\nSecond Citizen:\\nConsider you what services he has done for his country?\\n\\nFirst Cit'\n",
            "\n",
            "Next Char Predictions: \n",
            " \"sZOlFfv!O-jQEKDkbRJnHfLclM3g,:D;vt.zBo &&JETfwxIuxJX-OGmVFukpQIPZAkRfDivuDt,cwfiLGZtYlI!'zfua3F;XYbf\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRHS6b69PAeQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4457cab0-6c09-4887-d0bb-3d92d358f165"
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.173262\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRvpJ4m6PmJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSjvLJ0Sd3x7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAB0pvuSgbqg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "1a656dcb-d915-4974-c34d-be59837690d7"
      },
      "source": [
        "EPOCHS=10\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 7s 41ms/step - loss: 2.6684\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 7s 41ms/step - loss: 1.9649\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 7s 41ms/step - loss: 1.6959\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 7s 41ms/step - loss: 1.5467\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 7s 41ms/step - loss: 1.4582\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 7s 41ms/step - loss: 1.3990\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 7s 42ms/step - loss: 1.3537\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 7s 41ms/step - loss: 1.3146\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 7s 41ms/step - loss: 1.2794\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 7s 42ms/step - loss: 1.2480\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzLTARibgeBs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f419cc85-56e7-4ded-b491-7df08ca27e01"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_10'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi0hvAgIhb7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SOtgZCAhdmF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "89399f12-c443-423f-cfec-86d64ec8cc47"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faBA6p8Qijrz",
        "colab_type": "text"
      },
      "source": [
        "## The prediction loop\n",
        "The following code block generates the text:\n",
        "\n",
        "- It Starts by choosing a start string, initializing the RNN state and setting the number of characters to generate.\n",
        "\n",
        "- Get the prediction distribution of the next character using the start string and the RNN state.\n",
        "\n",
        "- Then, use a categorical distribution to calculate the index of the predicted character. Use this predicted character as our next input to the model.\n",
        "\n",
        "- The RNN state returned by the model is fed back into the model so that it now has more context, instead than only one character. After predicting the next character, the modified RNN states are again fed back into the model, which is how it learns as it gets more context from the previously predicted characters.\n",
        "\n",
        "![alt text](https://www.tensorflow.org/tutorials/text/images/text_generation_sampling.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neyTgteXhlbQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      # 缩放logits，temperature越大，输出结果越随机\n",
        "      predictions = predictions / temperature\n",
        "      \n",
        "      # shape=(batch_size, num_samples), 每个样本采样num_samples个候选输出\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSMx43EHhpFO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        },
        "outputId": "8146eeb4-7c68-4e73-ff5a-c30bfb6dc2cc"
      },
      "source": [
        "print(generate_text(model, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO: Camilloking fortune's together,\n",
            "And their woes have very little though, but witt thy temper:\n",
            "hold mine eyes, sweet Warwick Musicians,\n",
            "Then have the nobles ig you will\n",
            "consider of me.\n",
            "\n",
            "HORTENSIO:\n",
            "How shath a thousand to his friends the town;\n",
            "The ree cricket ridst a raib.\n",
            "\n",
            "FLORIZEL:\n",
            "No, it is a brinces to thine inr, I may know\n",
            "The duke-you Lord Angelo, why is it light:\n",
            "I mean, my lord, between your spittle, ears.\n",
            "\n",
            "MOPSA:\n",
            "Sir, it who is not sin to--\n",
            "To thrive away to thee to sure our foot.\n",
            "\n",
            "KING RITHARD III:\n",
            "Bishop by him to thy leadned thee.\n",
            "Farerel, for I had rather to thy duty--but thousands;\n",
            "For tenterman is full of mine own good with thick to inst thou living prince:\n",
            "The peerly,\n",
            "Make me with me.\n",
            "\n",
            "LUCIO:\n",
            "Nine Bianca say: the more indeed will feel\n",
            "a burgen Ghell; Nay, he comes himself.\n",
            "I am no power to deposed;\n",
            "And I beseech you then: if he be hath waken.\n",
            "\n",
            "QUEEN MARBARET:\n",
            "Thy gods, which is thy horse and here!\n",
            "\n",
            "PETRUCHIO:\n",
            "'Tis dropp roundly when his crown?\n",
            "\n",
            "GLOUCESTER:\n",
            "I cannot be no k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNjULzUVnucB",
        "colab_type": "text"
      },
      "source": [
        "## Advanced: Customized Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lZ28VO4hqjY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J--4HnLpn2YL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmEoRTR4n3XQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, target):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(inp)\n",
        "    loss = tf.reduce_mean(\n",
        "        tf.keras.losses.sparse_categorical_crossentropy(\n",
        "            target, predictions, from_logits=True))\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGgNNUyGn4Xb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        },
        "outputId": "dccfe995-4a7d-4d7a-ec0c-9b58a33c5f47"
      },
      "source": [
        "# Training step\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  # initializing the hidden state at the start of every epoch\n",
        "  # initally hidden is None\n",
        "  hidden = model.reset_states()\n",
        "\n",
        "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "    loss = train_step(inp, target)\n",
        "\n",
        "    if batch_n % 100 == 0:\n",
        "      template = 'Epoch {} Batch {} Loss {}'\n",
        "      print(template.format(epoch+1, batch_n, loss))\n",
        "\n",
        "  # saving (checkpoint) the model every 5 epochs\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "  print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "  print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.1732330322265625\n",
            "Epoch 1 Batch 100 Loss 2.3562841415405273\n",
            "Epoch 1 Loss 2.1361\n",
            "Time taken for 1 epoch 8.318379163742065 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.1341233253479004\n",
            "Epoch 2 Batch 100 Loss 1.9271557331085205\n",
            "Epoch 2 Loss 1.8353\n",
            "Time taken for 1 epoch 7.320577144622803 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.766493558883667\n",
            "Epoch 3 Batch 100 Loss 1.6475504636764526\n",
            "Epoch 3 Loss 1.6254\n",
            "Time taken for 1 epoch 7.356818199157715 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.6349270343780518\n",
            "Epoch 4 Batch 100 Loss 1.551010012626648\n",
            "Epoch 4 Loss 1.4897\n",
            "Time taken for 1 epoch 7.402777433395386 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.4808212518692017\n",
            "Epoch 5 Batch 100 Loss 1.4703172445297241\n",
            "Epoch 5 Loss 1.4158\n",
            "Time taken for 1 epoch 7.380358457565308 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.3682001829147339\n",
            "Epoch 6 Batch 100 Loss 1.3896385431289673\n",
            "Epoch 6 Loss 1.4341\n",
            "Time taken for 1 epoch 7.405625820159912 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.3454155921936035\n",
            "Epoch 7 Batch 100 Loss 1.3354101181030273\n",
            "Epoch 7 Loss 1.3599\n",
            "Time taken for 1 epoch 7.345083236694336 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.3095132112503052\n",
            "Epoch 8 Batch 100 Loss 1.3192970752716064\n",
            "Epoch 8 Loss 1.2859\n",
            "Time taken for 1 epoch 7.387158632278442 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.2091618776321411\n",
            "Epoch 9 Batch 100 Loss 1.3196126222610474\n",
            "Epoch 9 Loss 1.3001\n",
            "Time taken for 1 epoch 7.34596848487854 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.190628170967102\n",
            "Epoch 10 Batch 100 Loss 1.251729130744934\n",
            "Epoch 10 Loss 1.2893\n",
            "Time taken for 1 epoch 7.388072490692139 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JD7ve-rdn5cL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}